{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self,student,teacher):\n",
    "        super().__init__()\n",
    "        self.teacher=teacher\n",
    "        self.student=student\n",
    "    \n",
    "    def compile(self,optimizer,metrics,student_loss_fn,distillation_loss_fn,alpha=0.1,temperature=3):\n",
    "        super().compile(optimizer=optimizer,metrics=metrics)\n",
    "        self.student_loss_fn=student_loss_fn\n",
    "        self.distillation_loss_fn=distillation_loss_fn\n",
    "        self.alpha=alpha\n",
    "        self.temperature=temperature\n",
    "    \n",
    "    def train_step(self,data):\n",
    "        x,y=data # unpacking the data\n",
    "\n",
    "        # calculate the loss\n",
    "        teacher_predictions=self.teacher(x,training=False)\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions=self.student(x,training=True)\n",
    "            student_loss=self.student_loss_fn(y,student_predictions)\n",
    "            distillation_loss=self.distillation_loss_fn(tf.nn.softmax(teacher_predictions/self.temperature,axis=1),tf.nn.softmax(student_predictions/self.temperature,axis=1)*self.temperature**2)\n",
    "            loss=self.alpha*student_loss+(1-self.alpha)*distillation_loss\n",
    "        \n",
    "        # compute gradients\n",
    "        trainable_vars=self.student.trainable_variables\n",
    "        gradients=tape.gradient(loss,trainable_vars)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
    "\n",
    "        # update metrics \n",
    "        self.compiled_metrics.update_state(y,student_predictions)\n",
    "\n",
    "        # return performance\n",
    "        results={m.name: m.result() for m in self.metrics}\n",
    "        results.update({'student_loss':student_loss,'distillation_loss':distillation_loss})\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_step(self,data):\n",
    "        # unpack the data\n",
    "        x,y=data\n",
    "\n",
    "        # compute predictions\n",
    "        y_prediction=self.student(x,training=False)\n",
    "\n",
    "        # calculate the loss\n",
    "        student_loss=self.student_loss_fn(y,y_prediction)\n",
    "\n",
    "        # update the metrics\n",
    "        self.compiled_metrics.update_state(y,y_prediction)\n",
    "\n",
    "        # return performance\n",
    "        results={m.name:m.result() for m in self.metrics}\n",
    "        results.update({'student_loss':student_loss})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Teacher Model\n",
    "\n",
    "teacher=keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28,28,1)),\n",
    "        layers.Conv2D(256,(3,3),strides=(2,2),padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2,2),strides=(1,1),padding='same'),\n",
    "        layers.Conv2D(512,(3,3),strides=(2,2),padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name='teacher'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Student Model\n",
    "\n",
    "student=keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28,28,1)),\n",
    "        layers.Conv2D(16,(3,3),strides=(2,2),padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2,2),strides=(1,1),padding='same'),\n",
    "        layers.Conv2D(32,(3,3),strides=(2,2),padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10)\n",
    "    ],\n",
    "    name='student'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the student model for comparision\n",
    "student_scratch=keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset - MNIST - Digit Dataset\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalization - Feature Scaling\n",
    "x_train=x_train.astype('float32')/255.0\n",
    "\n",
    "# Reshape the data after normalization 1D to 2D\n",
    "x_train=np.reshape(x_train,(-1,28,28,1))\n",
    "\n",
    "x_test=x_test.astype('float32')/255.0\n",
    "x_test=np.reshape(x_test,(-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 11:57:36.110620: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 119s 63ms/step - loss: 0.1448 - sparse_categorical_accuracy: 0.9554\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 0.0920 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9774\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 118s 63ms/step - loss: 0.0733 - sparse_categorical_accuracy: 0.9789\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 114s 61ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9810\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.1148 - sparse_categorical_accuracy: 0.9724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11484106630086899, 0.9724000096321106]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the teacher model as usual \n",
    "\n",
    "# Compile the model\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "teacher.fit(x_train,y_train,epochs=5)\n",
    "\n",
    "# Evaluate the Model\n",
    "teacher.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 29s 15ms/step - sparse_categorical_accuracy: 0.9232 - student_loss: 0.2587 - distillation_loss: -1.5893\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 29s 16ms/step - sparse_categorical_accuracy: 0.9700 - student_loss: 0.0993 - distillation_loss: -1.5892\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 30s 16ms/step - sparse_categorical_accuracy: 0.9764 - student_loss: 0.0754 - distillation_loss: -1.5893\n",
      "313/313 [==============================] - 0s 997us/step - sparse_categorical_accuracy: 0.9823 - student_loss: 0.0565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9822999835014343, 0.0023729107342660427]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the distiller \n",
    "distiller=Distiller(student=student,teacher=teacher)\n",
    "\n",
    "# compile the distiller\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10\n",
    ")\n",
    "\n",
    "# Train the Student with Distiller\n",
    "distiller.fit(x_train,y_train,epochs=3)\n",
    "\n",
    "# Evaluate student with distiller on test dataset\n",
    "distiller.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2454 - sparse_categorical_accuracy: 0.9275\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0882 - sparse_categorical_accuracy: 0.9734\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0718 - sparse_categorical_accuracy: 0.9776\n",
      "313/313 [==============================] - 0s 877us/step - loss: 0.0573 - sparse_categorical_accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0572688914835453, 0.9815999865531921]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have trained Teacher Model - Training Acc (97.24%), Testing Acc (97.24%)\n",
    "# We have trained Student Model with Distillation - Training Acc (98.23%), Testing Acc (98.22%)\n",
    "\n",
    "# We have to train Student Model without Distillation\n",
    "\n",
    "student_scratch.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "student_scratch.fit(x_train,y_train,epochs=3)\n",
    "\n",
    "# Evaluate the Model\n",
    "student_scratch.evaluate(x_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies of Models\n",
    "\n",
    "1. Student with Distillation - High Accuracy - 98.23%\n",
    "2. Student without Distillation - 98.16%\n",
    "3. Teacher Model - 97.24%\n",
    "\n",
    "Distllation models can boost the performance of Teacher Model and Student Model without Distillation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
